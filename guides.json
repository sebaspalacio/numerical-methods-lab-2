{
  "incremental": {
    "title": "Incremental Search Guide",
    "body": [
      "**Inputs:** Function $f(x)$, Interval $[a, b]$, Initial Guess $x_0$, Delta $\\Delta x$, Max Steps.",
      "**Goal:** Finds a small bracket $[x_i, x_{i+1}]$ where the function changes sign.",
      "**Theory:** This method scans the domain starting at $x_0$ with steps of size $\\Delta x$. It stops when it finds an interval $[x_i, x_{i+1}]$ where the **Intermediate Value Theorem** applies, meaning $f(x_i) \\cdot f(x_{i+1}) < 0$.",
      "**Condition:** Requires $f(x)$ to be continuous."
    ]
  },
  "bisection": {
    "title": "Bisection Method Guide",
    "body": [
      "**Inputs:** Function $f(x)$, Interval $[a, b]$, Tolerance.",
      "**Goal:** Finds a root within a given bracket by repeatedly halving the interval.",
      "**Theorem (Bisection):** Given a continuous function $f$ on $[a, b]$, if **$f(a) \\cdot f(b) < 0$**, the method is guaranteed to converge to a root $x_v$ in the interval.",
      "**Convergence:** This method has **linear convergence** with an error bound of $$|x_v - x_m| < \\frac{b-a}{2^n}$$"
    ]
  },
  "falsePosition": {
    "title": "False Position Method Guide",
    "body": [
      "**Inputs:** Function $f(x)$, Interval $[a, b]$, Tolerance.",
      "**Goal:** A bracketing method, often faster than Bisection, that uses a linear interpolation (a \"secant\" line) to estimate the next root.",
      "**Theorem:** Requires the same conditions as Bisection: $f$ must be continuous and **$f(a) \\cdot f(b) < 0$**.",
      "**Note:** While often faster, it can converge very slowly if the function is highly convex or concave near the root."
    ]
  },
  "fixedPoint": {
    "title": "Fixed Point Method Guide",
    "body": [
      "**Inputs:** Function $f(x)$ (for finding root), Function $g(x)$ (for iteration $x = g(x)$), Initial Guess $x_0$, Tolerance.",
      "**Goal:** Finds a solution $x_v$ such that $x_v = g(x_v)$, which is a root of $f(x)$ if $f(x)$ is formulated as $f(x) = x - g(x)$.",
      "**Theorem (Fixed Point):** Convergence is guaranteed if $g(x)$ has a unique fixed point in $[a,b]$. This requires two conditions:",
      "1. $g(x)$ is continuous and maps $[a,b]$ into itself (i.e., for every $x \\in [a,b]$, $g(x)$ is also in $[a,b]$).",
      "2. The derivative $g'(x)$ exists and there is a constant $k < 1$ such that **$|g'(x)| \\le k$** for all $x \\in (a,b)$ (Contractive Mapping).",
      "**Convergence:** This method has **linear convergence**."
    ]
  },
  "newton_root": {
    "title": "Newton-Raphson Method Guide",
    "body": [
      "**Inputs:** Function $f(x)$, Initial Guess $x_0$, Tolerance. Optional: $f'(x)$.",
      "**Goal:** Finds a root using tangent lines. It is derived from a Fixed Point iteration where $g(x) = x - f(x)/f'(x)$.",
      "**Theorem:** Requires $f(x)$ to be twice-differentiable ($f \\in C^2[a,b]$).",
      "**Conditions:** The method requires $x_0$ to be \"sufficiently close\" to the root $x_v$, and the root must be simple (i.e., **$f'(x_v) \\neq 0$**). The app will warn if $f'(x)$ is near zero during iteration.",
      "**Convergence:** This method has **quadratic convergence** (order 2), which is very fast."
    ]
  },
  "secant": {
    "title": "Secant Method Guide",
    "body": [
      "**Inputs:** Function $f(x)$, Initial Guess $x_0$, Second Guess $x_1$, Tolerance.",
      "**Goal:** A modification of Newton's method that does not require an analytic derivative.",
      "**Theory:** It approximates the derivative $f'(x_n)$ using the slope of the secant line between the two previous points, $x_n$ and $x_{n-1}$.",
      "**Formula:** $$x_{n+1} = x_n - \\frac{f(x_n)(x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}$$",
      "**Convergence:** This method has **superlinear convergence** (order $\\approx 1.62$). It is slower than Newton but faster than linear methods."
    ]
  },
  "multiple": {
    "title": "Multiple Roots (Mod. Newton)",
    "body": [
      "**Inputs:** Function $f(x)$, Initial Guess $x_0$, Tolerance. Optional: $f'(x)$, $f''(x)$.",
      "**Goal:** A modification of Newton's method to find roots with multiplicity $m > 1$.",
      "**Theorem:** A root $x_v$ has multiplicity $m$ if $f(x_v) = f'(x_v) = ... = f^{(m-1)}(x_v) = 0$, but $f^{(m)}(x_v) \\neq 0$.",
      "**Formula:** This app uses the standard modified formula which restores quadratic convergence: $$x_{n+1} = x_n - \\frac{f(x_n)f'(x_n)}{[f'(x_n)]^2 - f(x_n)f''(x_n)}$$"
    ]
  },
  "gauss_simple": {
    "title": "Gaussian Elimination (Simple)",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** Solves $Ax=b$ by transforming the augmented matrix $[A|b]$ to upper triangular form $[U|c]$.",
      "**Result:** Shows the step-by-step forward elimination process. This app then uses backward substitution to find $x$.",
      "**Warning:** This method will fail if a zero pivot (a zero on the diagonal $A[k,k]$) is encountered during elimination."
    ]
  },
  "gauss_partial": {
    "title": "Gaussian Elimination (Partial Pivot)",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** A robust version of Gaussian Elimination.",
      "**Theory:** At each step $k$, the algorithm searches for the largest absolute value in the column $k$ (from row $k$ to $n$) and swaps its row with the pivot row $k$. This minimizes round-off error and avoids zero pivots.",
      "**Result:** Shows all elimination steps, including any row swaps."
    ]
  },
  "gauss_total": {
    "title": "Gaussian Elimination (Total Pivot)",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** The most numerically stable (but computationally expensive) pivoting strategy.",
      "**Theory:** At each step $k$, the algorithm searches the entire active submatrix (from $A[k,k]$ to $A[n,n]$) for the largest absolute value. It then performs *both* row and column swaps to move this element to the pivot position $A[k,k]$.",
      "**Result:** Shows all steps. The final solution $x$ must be \"un-permuted\" based on the column swaps."
    ]
  },
  "lu_gaussian": {
    "title": "LU Factorization (Simple)",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** Decomposes A into $A = LU$, where $L$ is lower triangular and $U$ is upper triangular.",
      "**Theory:** This factorization is the *result* of running Gaussian Elimination without pivoting, where the $U$ matrix is the final upper triangular matrix, and the $L$ matrix stores the multipliers $m_{ik}$ used during elimination.",
      "**Warning:** Fails if a zero pivot is encountered. Use LU Partial Pivot instead."
    ]
  },
  "lu_partial_pivot": {
    "title": "LU Factorization (Partial Pivot)",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** A stable factorization that finds $L$, $U$, and a permutation matrix $P$ such that $PA = LU$.",
      "**Theory:** This method performs row swaps (like `gauss_partial`) and tracks them in the $P$ matrix. The system $Ax=b$ is then solved as $LUx = Pb$ by solving $Ly = Pb$ (forward substitution) and $Ux = y$ (backward substitution).",
      "**Result:** Shows the final $L$, $U$, and $P$ matrices."
    ]
  },
  "doolittle": {
    "title": "Doolittle (LU) Factorization",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** Finds $A=LU$ where $L$ is **unit-diagonal** (1s on the diagonal).",
      "**Condition:** Requires all principal submatrices of A to be non-singular (i.e., no zero pivots)."
    ]
  },
  "crout": {
    "title": "Crout (LU) Factorization",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** Finds $A=LU$ where $U$ is **unit-diagonal** (1s on the diagonal).",
      "**Condition:** Requires all principal submatrices of A to be non-singular (i.e., no zero pivots)."
    ]
  },
  "cholesky": {
    "title": "Cholesky (LLT) Factorization",
    "body": [
      "**Inputs:** Matrix A, Vector b.",
      "**Goal:** Decomposes A into $A = LL^T$, where $L$ is a lower triangular matrix.",
      "**Theorem:** This method is extremely fast and stable, but has strict requirements.",
      "**Conditions:** The matrix A *must* be **symmetric** (i.e., $A = A^T$) and **positive-definite**. The app will check for symmetry; it will warn if the matrix is not positive-definite (i.e., if it requires taking the square root of a negative number)."
    ]
  },
  "jacobi": {
    "title": "Jacobi Method",
    "body": [
      "**Inputs:** Matrix A, Vector b, Initial Vector $x_0$, Tolerance.",
      "**Goal:** Solves $Ax=b$ iteratively by isolating the diagonal. $x^{(k)} = T_J x^{(k-1)} + C_J$.",
      "**Condition:** The method requires that there are **no zeros on the diagonal** of A.",
      "**Convergence:** Convergence is guaranteed if A is **diagonally dominant**. The app will check the spectral radius of the iteration matrix T, $\\rho(T)$. Convergence occurs if and only if $\\rho(T) < 1$."
    ]
  },
  "gauss_seidel": {
    "title": "Gauss-Seidel Method",
    "body": [
      "**Inputs:** Matrix A, Vector b, Initial Vector $x_0$, Tolerance.",
      "**Goal:** An improvement on Jacobi that uses the new $x_i$ values as soon as they are computed in the *same* iteration.",
      "**Condition:** Requires **no zeros on the diagonal** of A.",
      "**Convergence:** Often converges faster than Jacobi. Guaranteed if A is diagonally dominant or symmetric positive-definite. The app will check the spectral radius $\\rho(T)$; convergence occurs if $\\rho(T) < 1$."
    ]
  },
  "sor": {
    "title": "SOR (Successive Over-Relaxation)",
    "body": [
      "**Inputs:** Matrix A, Vector b, Initial Vector $x_0$, Tolerance, and Relaxation ($\\omega$).",
      "**Goal:** An accelerated version of Gauss-Seidel.",
      "**Condition:** Requires **no zeros on the diagonal**. The parameter $\\omega$ (\"omega\") controls the relaxation. If $\\omega=1$, SOR is identical to Gauss-Seidel. If $0 < \\omega < 1$, it is \"under-relaxation\". If $1 < \\omega < 2$, it is \"over-relaxation\".",
      "**Convergence:** Must have $0 < \\omega < 2$. The optimal $\\omega$ is problem-specific. The app will check $\\rho(T_\\omega)$."
    ]
  },
  "vandermonde": {
    "title": "Vandermonde Interpolation",
    "body": [
      "**Inputs:** A list of (x, y) data points.",
      "**Goal:** Finds the *single* polynomial $p(x) = a_n x^n + ... + a_0$ that passes through all $n+1$ points.",
      "**Theory:** This method builds a system of $n+1$ linear equations by plugging each point $(x_i, y_i)$ into the polynomial form. This creates the **Vandermonde Matrix** $V$, and solves $Va = y$ for the coefficients $a$.",
      "**Warning:** This method is **numerically unstable** for a large number of points ($n > 10$). The matrix $V$ becomes ill-conditioned. The PDF explicitly states this method is **\"no recomendable\"** (not recommended)."
    ]
  },
  "newton_interp": {
    "title": "Newton's Polynomial (Divided Diffs)",
    "body": [
      "**Inputs:** A list of (x, y) data points.",
      "**Goal:** Builds the *same* unique interpolating polynomial as Vandermonde, but in a more stable form.",
      "**Theory:** Uses the **Divided Differences** table. The coefficients $b_k = f[x_0, ..., x_k]$ are the diagonal entries of the table. The polynomial is $p(x) = b_0 + b_1(x-x_0) + b_2(x-x_0)(x-x_1) + ...$.",
      "**Advantage:** More stable than Vandermonde, and easy to add a new point (just add a new diagonal to the table)."
    ]
  },
  "lagrange": {
    "title": "Lagrange Polynomial",
    "body": [
      "**Inputs:** A list of (x, y) data points.",
      "**Goal:** A different formulation for the unique interpolating polynomial.",
      "**Theory:** Builds the polynomial as a sum of weighted \"basis polynomials\" $L_k(x)$. The formula is: $$p(x) = \\sum_{k=0}^{n} L_k(x) f(x_k)$$",
      "Each $L_k(x)$ is a polynomial of degree $n$ that is 1 at $x_k$ and 0 at all other $x_j$.",
      "**Advantage:** Conceptually simple and does not require solving a system of equations."
    ]
  },
  "spline_linear": {
    "title": "Linear Splines (Trazadores Lineales)",
    "body": [
      "**Inputs:** A list of (x, y) data points.",
      "**Goal:** Connects consecutive data points with straight lines (piecewise polynomials of degree 1).",
      "**Continuity:** The resulting spline $p(x)$ is **Continuous ($C^0$)**, meaning the pieces connect, but the derivative (slope) \"jumps\" at each point. This is *not* smooth."
    ]
  },
  "spline_quadratic": {
    "title": "Quadratic Splines (Trazadores Cuadráticos)",
    "body": [
      "**Inputs:** A list of (x, y) data points (at least 3).",
      "**Goal:** Connects points with parabolas (piecewise polynomials of degree 2).",
      "**Continuity:** The spline $p(x)$ is **$C^1$ Smooth**. This means it is continuous ($C^0$) and its first derivative $p'(x)$ is also continuous. The \"corners\" are smoothed out. This app enforces $p_1''(x_1) = 0$ as a boundary condition."
    ]
  },
  "spline_cubic": {
    "title": "Cubic Splines (Trazadores Cúbicos)",
    "body": [
      "**Inputs:** A list of (x, y) data points (at least 3).",
      "**Goal:** Connects points with cubic polynomials (degree 3). This is the most common and smoothest spline.",
      "**Continuity:** The spline $p(x)$ is **$C^2$ Smooth**. It is continuous ($C^0$), has a continuous first derivative ($C^1$), and a continuous second derivative ($C^2$).",
      "**Condition:** This app uses **\"Natural Splines\"**, which sets the second derivative at the endpoints to zero: $p''(x_0) = 0$ and $p''(x_n) = 0$."
    ]
  }
}
